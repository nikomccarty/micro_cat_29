{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7.1: Confidence intervals for microtubule catastrophe (35 pts)\n",
    "\n",
    "**[Dataset download](https://s3.amazonaws.com/bebi103.caltech.edu/data/gardner_time_to_catastrophe_dic_tidy.csv)**\n",
    "\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:07.442339Z",
     "start_time": "2019-11-21T19:17:45.347157Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as npg\n",
    "import hvplot\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import bokeh_catplot\n",
    "import bokeh \n",
    "import bokeh.io\n",
    "from bokeh.themes import Theme\n",
    "from bokeh.io import output_file, save, output_notebook\n",
    "import datashader as ds\n",
    "from holoviews.operation.datashader import datashade\n",
    "from bokeh.plotting import figure\n",
    "import numba\n",
    "from viz import pboc_style_bokeh\n",
    "from bebi103 import viz\n",
    "\n",
    "\n",
    "output_notebook()\n",
    "hv.extension('bokeh')\n",
    "#viz.set_plotting_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:07.467550Z",
     "start_time": "2019-11-21T19:18:07.454357Z"
    }
   },
   "outputs": [],
   "source": [
    "theme = Theme(json=pboc_style_bokeh())\n",
    "bokeh.io.curdoc().theme = theme\n",
    "hv.renderer('bokeh').theme = theme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refresh yourself about the microtubule catastrophe data we explored in homeworks [3.3](http://bebi103.caltech.edu.s3-website-us-east-1.amazonaws.com/2019a/content/homework/hw3/hw3.3.html) and [6.2](http://bebi103.caltech.edu.s3-website-us-east-1.amazonaws.com/2019a/content/homework/hw6/hw6.2.html). We will again work with this data set here.\n",
    "\n",
    "**a)** Remember that the confidence interval of the plug-in estimate of any statistical functional may be computed using bootstrapping. (This does not mean, however, that bootstrapping has great performance for any statistical functional; some have better behavior that others.) This includes the ECDF itself. Computing and plotting confidence intervals are implemented in the `bokeh_catplot.ecdf()` function. Plot the ECDFs of the catastrophe times for microtubules with labeled tubulin and for those with unlabeled tubulin including a confidence interval. In looking at the plot, do you think they two could be identically distributed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:07.493389Z",
     "start_time": "2019-11-21T19:18:07.478220Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mt = pd.read_csv('~/Downloads/gardner_time_to_catastrophe_dic_tidy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:07.512825Z",
     "start_time": "2019-11-21T19:18:07.505751Z"
    }
   },
   "outputs": [],
   "source": [
    "del(df_mt['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:07.545815Z",
     "start_time": "2019-11-21T19:18:07.522320Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:07.564411Z",
     "start_time": "2019-11-21T19:18:07.553221Z"
    }
   },
   "outputs": [],
   "source": [
    "mt_lab = df_mt[df_mt['labeled']== True]\n",
    "mt_non_lab = df_mt[df_mt['labeled']== False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:07.583842Z",
     "start_time": "2019-11-21T19:18:07.573791Z"
    }
   },
   "outputs": [],
   "source": [
    "import bokeh_catplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:07.596203Z",
     "start_time": "2019-11-21T19:18:07.589906Z"
    }
   },
   "outputs": [],
   "source": [
    "import colorcet as cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:11.458713Z",
     "start_time": "2019-11-21T19:18:07.603408Z"
    }
   },
   "outputs": [],
   "source": [
    "e = bokeh_catplot.ecdf(data = df_mt, \n",
    "                  cats = 'labeled', \n",
    "                  val = 'time to catastrophe (s)',\n",
    "                  conf_int = True, \n",
    "                  palette = cc.glasbey_dark[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:11.682547Z",
     "start_time": "2019-11-21T19:18:11.482856Z"
    }
   },
   "outputs": [],
   "source": [
    "bokeh.io.show(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: It looks like both distributions are very similar. From looking at the plot, because the 95% confidence intervals overlap so much I would say that there's no difference between the labeled and unlabeled tubulin. \n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Compute confidence intervals for the plug-in estimate for the mean time to catastrophe for each of the two conditions and comment on the result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:11.704649Z",
     "start_time": "2019-11-21T19:18:11.695299Z"
    }
   },
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def draw_bs_sample(data):\n",
    "    \"\"\"Draw a bootstrap sample from a 1D data set.\"\"\"\n",
    "\n",
    "    bs_sample = np.random.choice(data, size=len(data))\n",
    "    \n",
    "    return bs_sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:11.729573Z",
     "start_time": "2019-11-21T19:18:11.716544Z"
    }
   },
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def draw_bs_rep_mean(data, n_reps):\n",
    "    \"\"\"\n",
    "    Draw bootstrap replicates for the mean. \n",
    "    \"\"\"\n",
    "    means_bs_reps = np.empty(n_reps)\n",
    "    \n",
    "    for i in range(n_reps):\n",
    "        #bs_sample = \n",
    "        means_bs_reps[i] = np.mean(draw_bs_sample(data))\n",
    "        \n",
    "    return means_bs_reps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the values corresponding to the labeled and unlabeled tubulin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:11.755793Z",
     "start_time": "2019-11-21T19:18:11.746424Z"
    }
   },
   "outputs": [],
   "source": [
    "mt_lab_tcat = mt_lab['time to catastrophe (s)'].values\n",
    "mt_non_lab_tcat = mt_non_lab['time to catastrophe (s)'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both distributions in `np.arrays` we can now go ahead and compute bootstrap replicates for the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:13.546673Z",
     "start_time": "2019-11-21T19:18:11.766735Z"
    }
   },
   "outputs": [],
   "source": [
    "mt_lab_mean =  draw_bs_rep_mean(mt_lab_tcat, n_reps= np.int(1e4))\n",
    "mt_non_lab_mean =  draw_bs_rep_mean(mt_non_lab_tcat, n_reps= np.int(1e4))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going forward we can now get the 95% confidence intervals for the mean of both distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:13.578711Z",
     "start_time": "2019-11-21T19:18:13.559263Z"
    }
   },
   "outputs": [],
   "source": [
    "mt_lab_mean_ci = np.percentile(mt_lab_mean, [2.5, 97.5])\n",
    "mt_non_lab_mean_ci = np.percentile(mt_non_lab_mean, [2.5, 97.5])\n",
    "\n",
    "print(\"\"\"Mean time to catastrophe 95 % confidence interval [{0:.2f}, {1:.2f}]\n",
    "for the labeled microtubules. \\n\"\"\".format(*mt_lab_mean_ci))\n",
    "\n",
    "print(\"\"\"Mean time to catastrophe 95 % confidence interval [{0:.2f}, {1:.2f}]\n",
    "for the unlabeled microtubules.\"\"\".format(*mt_non_lab_mean_ci))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does look that the mean time to catastrophe is similar in the higher end of the values but diverges quite a bit on the lower end of the distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**c)** Test the hypothesis the distribution of catastrophe times for microtubules with labeled tubulin is the same as that for unlabeled tubulin. Think carefully about a good test statistic and justify your choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Let's use a permutation NHST recipe.\n",
    "\n",
    "1. State the null hypothesis (more- or less already stated): The distributions of the labeled and unlabeled tubulin are the same. \n",
    "\n",
    "2. Define a test statistic: Difference of means squared weighted by the the product of the standard deviations. That is:\n",
    "\n",
    "$$\n",
    "\\text{D} = \\frac{ \\left( \\bar{t_1} - \\bar{t_2} \\right) ^2}{\\sigma_{t_1} \\sigma_{t_2}}\n",
    "$$\n",
    "\n",
    "We chose this statistic because it takes into account both the first and second moments of the distribution.\n",
    "\n",
    "3. Rejection region : We'll choose an alpha of $1x10^-3$. \n",
    "\n",
    "4. Simulate data acquisition: Okay ! \n",
    "\n",
    "But before we apply the bootstrap replicates of our test statistic, let's think about why is this is a good measure of the difference.\n",
    "\n",
    "We know that the first and second moments of the exponential distribution are just $1/\\beta$ and $1\\beta ^2$ respectively. We know the PDF is a monotonically decreasing function and therefore be more interested in the rightward tail of the distribution. From this assumption it is okay to assume that for example, a difference of the means would in essence be a good test statistic by itself. Moreover, because we want to **test whether the distributions are different** and not just the means, we want to include that in the test statistic we use. \n",
    "\n",
    "We acknowledge the fact that the test statistic *might* be biased, and that we may be a more formal way to characterize the difference in exponential distributions, but we'll go with this for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:13.601651Z",
     "start_time": "2019-11-21T19:18:13.590680Z"
    }
   },
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def diff_means(x, y): \n",
    "    \n",
    "    diff_means = (np.mean(x) - np.mean(y))**2 / (np.std(x) - np.std(y))\n",
    "    \n",
    "    return diff_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's concatenate and scramble both samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:13.627725Z",
     "start_time": "2019-11-21T19:18:13.617485Z"
    }
   },
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def draw_perm_sample(x, y):\n",
    "    \"\"\"Generate a permutation sample.\"\"\"\n",
    "    concat_data = np.concatenate((x, y))\n",
    "    np.random.shuffle(concat_data)\n",
    "\n",
    "    return concat_data[:len(x)], concat_data[len(x):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:13.647026Z",
     "start_time": "2019-11-21T19:18:13.634452Z"
    }
   },
   "outputs": [],
   "source": [
    "def draw_perm_reps(x, y, stat_fun, size=1):\n",
    "    \"\"\"Generate array of permuation replicates.\"\"\"\n",
    "    return np.array([stat_fun(*draw_perm_sample(x, y)) for _ in range(size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:13.665370Z",
     "start_time": "2019-11-21T19:18:13.654011Z"
    }
   },
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def draw_perm_reps_diff_mean(x, y, size=1):\n",
    "    \"\"\"Generate array of permuation replicates.\"\"\"\n",
    "    out = np.empty(size)\n",
    "    for i in range(size):\n",
    "        x_perm, y_perm = draw_perm_sample(x, y)\n",
    "        out[i] = diff_means(x_perm, y_perm)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to resolve values below  to a $1x10^-3$ p-value, we will make $1x10^4$ bootstrap reps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:16.049881Z",
     "start_time": "2019-11-21T19:18:13.675729Z"
    }
   },
   "outputs": [],
   "source": [
    "perm_reps = draw_perm_reps_diff_mean(mt_lab_tcat,mt_non_lab_tcat,size = np.int(1e4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:16.068361Z",
     "start_time": "2019-11-21T19:18:16.055210Z"
    }
   },
   "outputs": [],
   "source": [
    "diff_mean = diff_means(mt_lab_tcat, mt_non_lab_tcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:16.085497Z",
     "start_time": "2019-11-21T19:18:16.078505Z"
    }
   },
   "outputs": [],
   "source": [
    "pval = np.sum(perm_reps >= diff_mean) / len(perm_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:16.111242Z",
     "start_time": "2019-11-21T19:18:16.098980Z"
    }
   },
   "outputs": [],
   "source": [
    "pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that indeed there is no difference in the distributions when one takes into a account both the first and second moments. \n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**d)** In part (b), you used bootstrapping to compute a confidence interval for the plug-in estimate for the mean time to catastrophe. As is often (though definitely not always) the case, we could use a theoretical result to construct a confidence interval. The central limit theorem states that the mean, which is the sum of many processes, should be approximately Normally distributed. We will not derive it here, but the mean and variance of that Normal distribution are approximately\n",
    "\n",
    "\\begin{align}\n",
    "&\\mu = \\bar{x},\\\\[1em]\n",
    "&\\sigma^2 = \\frac{1}{n(n-1)}\\sum_{i=1}^n (x_i - \\bar{x})^2,\n",
    "\\end{align}\n",
    "\n",
    "where $\\bar{x}$ is the arithmetic mean of the data points. To compute a confidence interval of the mean, then, you can compute the interval over which 95% of the probability mass of the above described Normal distribution lies. \n",
    "\n",
    "Compute this approximate confidence interval and compare it to the result you got in part (b). *Hint*: You can use the `scipy.stats` package to conveniently get intervals for named distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:16.148617Z",
     "start_time": "2019-11-21T19:18:16.127530Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:16.221562Z",
     "start_time": "2019-11-21T19:18:16.160290Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_lab_ci, _, _ = scipy.stats.bayes_mvs(mt_lab_tcat)\n",
    "mean_unlab_ci, _, _ = scipy.stats.bayes_mvs(mt_non_lab_tcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:16.249515Z",
     "start_time": "2019-11-21T19:18:16.238929Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\"\"Mean time to catastrophe 95 % confidence interval [{0:.2f}, {1:.2f}]\n",
    "for the labeled microtubules using scipy. \\n\"\"\".format(*list(mean_lab_ci)[1]))\n",
    "\n",
    "print(\"\"\"Mean time to catastrophe 95 % confidence interval [{0:.2f}, {1:.2f}]\n",
    "for the unlabeled microtubules using scipy.\"\"\".format(*list(mean_unlab_ci)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:16.263177Z",
     "start_time": "2019-11-21T19:18:16.254975Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\"\"Mean time to catastrophe 95 % confidence interval [{0:.2f}, {1:.2f}]\n",
    "for the labeled microtubules using bootstrap. \\n\"\"\".format(*mt_lab_mean_ci))\n",
    "\n",
    "print(\"\"\"Mean time to catastrophe 95 % confidence interval [{0:.2f}, {1:.2f}]\n",
    "for the unlabeled microtubules using bootstrap.\"\"\".format(*mt_non_lab_mean_ci))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the results are fairly similar. In both cases the bootstrap confidence intervals are broader than the ones calculated with scipy. From reading the docs of the `scipy.stats.bayes_mvs` it seems that there's quite a bit of theory behind the calculation and the implementation of the confidence intervals, the paper shows that the mean is $\\text{t}$ distributed, and the standard deviation and variance are $\\text{Gamma}$ distributed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T10:12:55.190597Z",
     "start_time": "2019-11-19T10:12:55.177039Z"
    }
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Write a function with call signature `ecdf(x, data)`, which computes the value of the ECDF built from the one-dimensional array `data` at arbitrary points `x`. That is, `x` can be an array. Write this function also helps cement in your mind what an ECDF is and will be useful in part (f).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** Sorry, but the x as an argument seems a little ambiguous. If I understood correctly we want to extract the of a given distribution at specified percentiles. Thus, instead of x, I will call the argument percentiles. By default it will return the 2nd to the 98th percentile in linearly spaced intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:16.318355Z",
     "start_time": "2019-11-21T19:18:16.309637Z"
    }
   },
   "outputs": [],
   "source": [
    "def ecdf(data, percentiles = np.linspace(2, 98, 21)):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate an ECDF along specified percentiles. \n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    percentiles (array-like): list of percentiles.\n",
    "    data (array-like): distribution to make an ECDF from. \n",
    "    \n",
    "    Returns \n",
    "    ---------\n",
    "    \n",
    "    sorted_sel_data : sorted dataset at specified percentiles.\n",
    "    \"\"\"\n",
    "    \n",
    "    sorted_data = np.sort(data)\n",
    "    \n",
    "    n = len(data)\n",
    "    \n",
    "    # Get the indices for specified percentiles\n",
    "    ixs = [ int(pcnt/100*n) for pcnt in percentiles]\n",
    "    \n",
    "    # Extract the values for the specified indices\n",
    "    sorted_sel_data = sorted_data[ixs]\n",
    "    \n",
    "    return sorted_sel_data, percentiles/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:16.335155Z",
     "start_time": "2019-11-21T19:18:16.328406Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_mt_lab_mean, ecdf_mt_lab_mean = ecdf(mt_lab_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:16.350404Z",
     "start_time": "2019-11-21T19:18:16.342332Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_mt_unlab_mean, ecdf_un_mt_lab_mean = ecdf(mt_non_lab_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:16.382806Z",
     "start_time": "2019-11-21T19:18:16.356376Z"
    }
   },
   "outputs": [],
   "source": [
    "labeled = hv.Curve((sorted_mt_lab_mean,\n",
    "                    ecdf_mt_lab_mean), label = 'labeled').opts(xlabel= 'mean time to catastrophe',\n",
    "                                                      ylabel = 'ECDF')\n",
    "\n",
    "unlabeled = hv.Curve((sorted_mt_unlab_mean,\n",
    "                    ecdf_mt_lab_mean), label = 'unlabeled').opts(xlabel= 'mean time to catastrophe',\n",
    "                                                      ylabel = 'ECDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:16.803335Z",
     "start_time": "2019-11-21T19:18:16.388046Z"
    }
   },
   "outputs": [],
   "source": [
    "(labeled*unlabeled).opts(padding = 0.1,\n",
    "                         width = 600,\n",
    "                         show_grid = True,\n",
    "                         legend_position = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite there is a potentially similar distribution, the bootstrap replicates of the mean have lower values for the unlabeled distribution, though it converges at higher values of the distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T09:53:38.304582Z",
     "start_time": "2019-11-19T09:53:38.293205Z"
    }
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)** In part (a), you used bootstrapping to compute a confidence interval on the ECDF. As is often (though definitely not always) the case, we could use a theoretical result to construct a confidence interval. We could alternatively use the **Dvoretzky-Kiefer-Wolfowitz Inequality** (DKW) to compute confidence intervals for an ECDF. The DKW inequality puts an upper bound on the maximum distance between the ECDF $\\hat{F}(x)$ and the generative CDF $F(x)$. It states that, for any $\\epsilon > 0$,\n",
    "\n",
    "\\begin{align}\n",
    "P\\left(\\mathrm{sup}_x \\left|F(x) - \\hat{F}(x)\\right| > \\epsilon\\right) \\le 2\\mathrm{e}^{-2 n \\epsilon^2},\n",
    "\\end{align}\n",
    "\n",
    "where $n$ is the number of points in the data set. We could use this inequality to set up a bound for the confidence interval. To construct the bound on the $100 \\times (1-\\alpha)$ percent confidence interval, we specify that\n",
    "\n",
    "\\begin{align}\n",
    "\\alpha = 2\\mathrm{e}^{-2 n \\epsilon^2},\n",
    "\\end{align}\n",
    "\n",
    "which gives\n",
    "\n",
    "\\begin{align}\n",
    "\\epsilon = \\sqrt{\\frac{1}{2n}\\,\\log \\frac{2}{\\alpha}}.\n",
    "\\end{align}\n",
    "\n",
    "Then, the lower bound on the confidence interval is\n",
    "\n",
    "\\begin{align}\n",
    "L(x) = \\max\\left(0, \\hat{F}(x) - \\epsilon\\right),\n",
    "\\end{align}\n",
    "\n",
    "and the upper bound is \n",
    "\n",
    "\\begin{align}\n",
    "U(x) = \\min\\left(1, \\hat{F}(x) + \\epsilon\\right).\n",
    "\\end{align}\n",
    "\n",
    "Note that this is not strictly speaking a confidence interval, but rather a set of bounds for where the confidence interval can lie (it's the DKW **in**equality after all).\n",
    "\n",
    "Plot the upper and lower bounds for the 95% confidence interval as computed from the DKW inequality for the  microtubule catastrophe data and comment on what you see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** : All right, let code a function to get the DKW conf interval bounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:16.834320Z",
     "start_time": "2019-11-21T19:18:16.817814Z"
    }
   },
   "outputs": [],
   "source": [
    "def dkw_ecdf_conf_int(data, alpha = 0.05):\n",
    "    \n",
    "    sorted_data = np.sort(data)\n",
    "    \n",
    "    ecdf = np.linspace(0,1, len(data))\n",
    "    \n",
    "    n = len(data)\n",
    "    \n",
    "    eps = np.sqrt( 1/(2*n) * np.log((2/alpha)))\n",
    "    \n",
    "    dkw_low_bound = ecdf - eps \n",
    "    \n",
    "    dkw_upper_bound = ecdf + eps \n",
    "    \n",
    "    return dkw_low_bound, dkw_upper_bound\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mt_lab_tcat = mt_lab['time to catastrophe (s)'].values\n",
    "mt_non_lab = mt_non_lab['time to catastrophe (s)'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the DKW bounds for both distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:16.884610Z",
     "start_time": "2019-11-21T19:18:16.867358Z"
    }
   },
   "outputs": [],
   "source": [
    "dkw_low_lab, dkw_upper_lab = dkw_ecdf_conf_int(mt_lab_tcat)\n",
    "dkw_low_unlab, dkw_upper_unlab = dkw_ecdf_conf_int(mt_non_lab_tcat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a sorted version of the arrays to make plots using the `bebi103.viz.fill_between` function. (Sorry for all the different variables!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:16.903894Z",
     "start_time": "2019-11-21T19:18:16.896658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sort the time to catastrophe values for the labeled microtubules\n",
    "sorted_mt_lab_tcat = np.sort(mt_lab_tcat)\n",
    "\n",
    "# Sort the t_cat for the non-labeled microtubules\n",
    "sorted_mt_non_lab_tcat = np.sort(mt_non_lab_tcat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Nowe we can go ahead and overlay the bootstrap ECDF from using `bokeh_catplot` and the theoretical bounds from the DKW calculation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the labeled ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:17.171207Z",
     "start_time": "2019-11-21T19:18:16.911899Z"
    }
   },
   "outputs": [],
   "source": [
    "l = bokeh_catplot.ecdf(data = mt_lab, \n",
    "                  cats = 'labeled', \n",
    "                  val = 'time to catastrophe (s)',\n",
    "                  conf_int = True, \n",
    "                  palette = cc.glasbey_dark[-1:],\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:17.233513Z",
     "start_time": "2019-11-21T19:18:17.180871Z"
    }
   },
   "outputs": [],
   "source": [
    "l = viz.fill_between(sorted_mt_lab_tcat,\n",
    "                      dkw_low_lab,\n",
    "                      sorted_mt_lab_tcat,\n",
    "                      dkw_upper_lab,\n",
    "                      p = l,\n",
    "                      fill_color = cc.glasbey_dark[-1],\n",
    "                      line_color = cc.glasbey_dark[-1],\n",
    "                      fill_alpha= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:17.351180Z",
     "start_time": "2019-11-21T19:18:17.243113Z"
    }
   },
   "outputs": [],
   "source": [
    "# show plot for labeled microtubules\n",
    "bokeh.io.show(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool! This is freaking art!! Haha ok back to science... We can see that in the most part the DKW bound really captures an upper bound on the bootstrap confidence intervals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:17.576343Z",
     "start_time": "2019-11-21T19:18:17.360042Z"
    }
   },
   "outputs": [],
   "source": [
    "un = bokeh_catplot.ecdf(data = mt_non_lab, \n",
    "                  cats = 'labeled', \n",
    "                  val = 'time to catastrophe (s)',\n",
    "                  conf_int = True, \n",
    "                  palette = cc.glasbey_dark[-2:],\n",
    "                  #p = un \n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:17.620453Z",
     "start_time": "2019-11-21T19:18:17.592422Z"
    }
   },
   "outputs": [],
   "source": [
    "un = viz.fill_between(sorted_mt_non_lab_tcat,\n",
    "                      dkw_low_unlab,\n",
    "                      sorted_mt_non_lab_tcat,\n",
    "                      dkw_upper_unlab,\n",
    "                      p = un,\n",
    "                      fill_color = cc.glasbey_dark[-2],\n",
    "                      line_color = cc.glasbey_dark[-2],\n",
    "                      fill_alpha= 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:18:17.852726Z",
     "start_time": "2019-11-21T19:18:17.640571Z"
    }
   },
   "outputs": [],
   "source": [
    "# show plot for unlabeled microtubules\n",
    "bokeh.io.show(un)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that we note is that in the upper tail of the distribution (towards higher values) the DKW bounds seem to be broader, but it might just be a visual effect of the fact that there are less points from which to sample... These results give some intiution on the construction of frequentist confidence intervals: If the experiment is repeated over and over again the estimate for the parameter (in this case, the ECDF itself) will lie between the bounds of the 95% confidence interval (for 95% of the experiments). Thus, if there are very few points for the a given region of the ECDF, we will have more uncertaintity the of where the ECDF will lie in for 95% of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attributions**: All of the members of the group contributed to the discussed and solved the individual exercises. The final notebook was edited by Manu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
